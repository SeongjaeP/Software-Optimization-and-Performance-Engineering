## Write - up 1
    In the assembly code, the compiler chooses to adjust the base pointer to point to the end of the array (i.e., a - SIZE) and then use positive offsets to access each element. This allows the memory accesses to remain within a fixed, small signed offset range, which can be encoded efficiently in instructions.

    If the compiler had kept the base pointer unchanged (i.e., a[0] corresponds to offset 0), then accessing a[i] for i up to SIZE - 1 would require larger immediate values as the index increases. Many architectures (including x86 and ARM) restrict the range of immediate values in memory access instructions (e.g., 8-bit or 12-bit signed offset). By shifting the base pointer lower, all offsets become small and positive (or at least within a small signed range), allowing more efficient instruction encoding.

    Therefore, the compiler rewrites the loop using a negative start index to reduce the instruction size and improve performance by avoiding large offset constants or additional address computation instructions.



## Wrtie - up 3
    The compiler generates dramatically different assembly because the use of __builtin_assume_aligned() allows it to assume that the input arrays are aligned in memory. This assumption enables the compiler to apply more aggressive optimizations such as loop vectorization using SIMD instructions like AVX2, and to use aligned memory operations (vmovdqa) instead of unaligned ones (vmovdqu). Without this hint, the compiler must conservatively assume the pointers may not be aligned, which inhibits many low-level optimizations.


## Write - up 4
    - Why the Assembly Does Not Include Vector Instructions
        The primary reason the compiler did not generate instructions with vector registers is its inability to definitively rule out a potential memory dependency at compile time.

            - Conservative Compiler Analysis: Vectorization (SIMD) requires the compiler to prove that parallel execution of multiple operations will yield the exact same result as sequential execution. In the given code, a[i] = b[i + 1], the read address (&b[i+1]) and the write address (&a[i]) are offset from each other. This misalignment creates ambiguity for the compiler. It conservatively assumes a worst-case scenario where the source and destination arrays (a and b) might overlap in memory in such a way that a write to an element of a could corrupt a value in b that is needed for a subsequent read (a data hazard).

            - Role of the restrict Keyword: While the restrict keyword is a promise from the programmer that the memory blocks pointed to by a and b do not overlap, a compiler's auto-vectorizer can still be conservative when faced with complex or misaligned access patterns. It may fail to leverage the restrict guarantee if its analysis patterns are not sophisticated enough to handle the +1 offset.

            - Comparison to a Simpler Case: If the code were a[i] = b[i], the compiler could easily vectorize it. The aligned access pattern makes it trivial to prove that there are no dependencies, so it would generate vector instructions to copy chunks of memory directly from b to a. The b[i+1] offset complicates this proof, causing the compiler to revert to safer, non-vectorized (scalar) instructions.

    - Performance Impact of Vectorization
        Yes, the code would run significantly faster if it were vectorized.

            - Theoretical Performance Gain: The operation is essentially a memory copy. The scalar code generated by the compiler moves data one byte at a time. In contrast, modern CPUs feature SIMD (Single Instruction, Multiple Data) units. For instance, a CPU with 256-bit AVX2 (Advanced Vector Extensions 2) registers could perform this operation on multiple data elements simultaneously.

            - Calculation: A 256-bit register can hold 32 individual 8-bit integers (uint8_t), since 256รท8=32.
                This means a single vector instruction could execute the equivalent of 32 iterations of the loop at once. This would lead to a theoretical speedup of up to 32x (though practical gains would be slightly lower due to instruction overhead).

            - Hardware Capabilities: Modern CPUs are equipped with instructions to handle unaligned memory access efficiently (e.g., vmovdqu on x86). Therefore, the b[i+1] offset is not a significant barrier for the hardware itself. The bottleneck is not the CPU's ability to execute the vectorized operation, but the compiler's ability to safely generate the corresponding vector code during its static analysis phase.

## Write - up 5
    % clang -O3 example4.c -o example4; ./example4
        The decimal floating point sum result is: 11.667578
        The raw floating point sum result is: 0x1.755cccec10aa5p+3

    % clang -O3 -ffast-math example4.c -o example4; ./example4
        The decimal floating point sum result is: 11.667578
        The raw floating point sum result is: 0x1.755cccec10aa2p+3

    In computer floating-point math, the order of operations matters. Each addition can create a tiny rounding error. By changing the order of the additions, the way these small errors accumulate also changes, leading to a slightly different final result.